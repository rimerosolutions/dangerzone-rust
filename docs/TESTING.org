#+TITLE: TESTING

All the testing is manual at this time.

Over time, it's been painful to test software releases due to the lack of an automated process.
- Occasional regression bugs
- Obvious minor issues missed
  - Container solutions specific considerations
  - Operating system specific considerations    
- Missing French translations
- Etc.  

* Testing details

- *Test Environments*
  - CPU architectures
    - amd64: Few Linux distributions, Mac OS and Windows 11
    - arm64: Linux only (Alpine Linux Virtual machine running on an amd64 machine, per arm64 testing instructions to follow)
  - *Container solutions*
    - [[https://www.docker.com/][Docker]] (Windows amd64, Linux arm64)
    - [[https://podman.io/][Podman]] (Linux amd64)
    - [[https://github.com/lima-vm/lima][Lima]] (Mac OS amd64)
  - *Components*
    - =entrusted-client= (Desktop and command-line interfaces)
    - =entrusted-webserver= (Online Web service with a Web interface)
      - Standalone binary testing
      - Live CD testing
    - =entrusted-webclient= (Command-line client for =entrusted-webserver=)
- *Test data*
  - Mostly sample files from the Entrusted [[https://github.com/rimerosolutions/entrusted][GitHub repository]] (=test_data= folder)
  - Additionally medium-size documents such as the [[https://github.com/void-linux/void-docs/files/4985723/handbook.pdf][Void Linux handbook PDF version]] (~136 pages)
- *Test machines technical specifications*
  - RAM: At least 8 GB of RAM
  - CPU: At least 2 cpus

*Notes*: We always need to test with =Fedora= under Linux as container behavior/configuration might differ and that will impact the conversion outcomes ([[https://www.redhat.com/en/topics/linux/what-is-selinux][SELinux]], etc.).

* Testing approach

It is unrealistic at this time to discard manual testing.

Generally speaking, there are always problems easier to spot with manual testing, especially as it pertains to visual interfaces.

** What can we automate?

We can start by focusing on the test dataset in the =test_data= folder.

** How do we automate?

Initially, this can be a single scenario via [[https://github.com/cucumber-rs/cucumber][cucumber-rs]] (=entrusted-cli=)
- We loop through all the test files in =test_data=, as described in a [[https://cucumber.io/docs/gherkin/reference][Gherkin]] feature file
- For each file, we perform a conversion via the =entrusted-cli= program
- We ensure that exit codes are OK
- We cleanup any temporary files created

Note:
- This could have been all tested without Cucumber and may need to be revisited to justify the usage of Cucumber.
- =cucumber-rs= does seem to pull quite a lot of dependencies...

